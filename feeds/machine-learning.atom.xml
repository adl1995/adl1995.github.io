<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>/home/adeel - Machine Learning</title><link href="https://adl1995.github.io/" rel="alternate"></link><link href="https://adl1995.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://adl1995.github.io/</id><updated>2017-11-13T10:54:00+01:00</updated><entry><title>An overview of activation functions used in neural networks</title><link href="https://adl1995.github.io/an-overview-of-activation-functions-used-in-neural-networks.html" rel="alternate"></link><published>2017-11-13T10:54:00+01:00</published><updated>2017-11-13T10:54:00+01:00</updated><author><name>Adeel Ahmad</name></author><id>tag:adl1995.github.io,2017-11-13:/an-overview-of-activation-functions-used-in-neural-networks.html</id><summary type="html">&lt;p&gt;An activation function is used to introduce non-linearity in an artificial neural network. It allows us to model a class label or score that varies non-linearly with independent variables. Non-linearity means that the output cannot be replicated from a linear combination of inputs; this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;. On the other hand, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective only up to a single&amp;nbsp;layer.&lt;/p&gt;
&lt;p&gt;To allow backpropagation through the network, the selected activation function should be differentiable. This property is required to compute …&lt;/p&gt;</summary><content type="html">&lt;p&gt;An activation function is used to introduce non-linearity in an artificial neural network. It allows us to model a class label or score that varies non-linearly with independent variables. Non-linearity means that the output cannot be replicated from a linear combination of inputs; this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;. On the other hand, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective only up to a single&amp;nbsp;layer.&lt;/p&gt;
&lt;p&gt;To allow backpropagation through the network, the selected activation function should be differentiable. This property is required to compute the gradients which allows us to tune the network weights. The non-linear functions are continuous and transform the input (normally &lt;a href="http://cs231n.github.io/neural-networks-2/#datapre"&gt;zero-centered&lt;/a&gt;, however, these values get beyond their original scale once they get multiplied with their respective weights) in the range $(0, 1)$, $(-1, 1)$, etc. In a neural network, it is possible for some neurons to have linear activation functions, but they must be accompanied by neurons with non-linear activation functions in some other part of the same&amp;nbsp;network.&lt;/p&gt;
&lt;p&gt;Although any non-linear function can be used as an activation function, in practice, only a small fraction of these are used. The sections below describe various activation functions. These are accompanied with a Python snippet to plot them using &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; and &lt;a href="https://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;:&lt;/p&gt;
&lt;h2&gt;Binary&amp;nbsp;step&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(z^i_j) = \begin{cases} 0  \hspace{1em} \text{if} \hspace{0.3em} z^i_j &amp;lt; 0 \ 1 \hspace{1em} \text{if} \hspace{0.3em} z^i_j &amp;gt; 0&amp;nbsp;\end{cases}$$&lt;/p&gt;
&lt;p&gt;A binary step function is generally used in the &lt;a href="https://en.wikipedia.org/wiki/Perceptron"&gt;Perceptron linear classifier&lt;/a&gt;. It thresholds the input values to $1$ and $0$, if they are greater or less than zero,&amp;nbsp;respectively.&lt;/p&gt;
&lt;p&gt;This activation function is useful when the input pattern can only belong to one or two groups, that is, binary&amp;nbsp;classification.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="step" src="https://adl1995.github.io/images/plots/step.svg"&gt;&lt;/p&gt;
&lt;h2&gt;$\tanh$&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) =&amp;nbsp;\tanh(x^i_j)$$&lt;/p&gt;
&lt;p&gt;The $\tanh$ non-linearity compresses the input in the range $(-1, 1)$. It provides an output which is zero-centered. So, large negative values are mapped to negative outputs. Similarly, zero-valued inputs are mapped to near zero&amp;nbsp;outputs.&lt;/p&gt;
&lt;p&gt;Also, the gradients for $\tanh$ are steeper than sigmoid, but it suffers from the &lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"&gt;vanishing gradient problem&lt;/a&gt;. $\tanh$ is commonly referred to as the scaled version of sigmoid, and generally this equation holds: $\tanh(x) = 2 \sigma(2x) -&amp;nbsp;1$&lt;/p&gt;
&lt;p&gt;An alternative equation for the $\tanh$ activation function&amp;nbsp;is:&lt;/p&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = \frac{2}{1+\exp(-2x^i_j)} -&amp;nbsp;1$$&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="tanh" src="https://adl1995.github.io/images/plots/tanh.svg"&gt;&lt;/p&gt;
&lt;h2&gt;ArcTan&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) =&amp;nbsp;\tanh^{-1}(x^i_j)$$&lt;/p&gt;
&lt;p&gt;This activation function maps the input values in the range $(-\pi/2, \pi/2)$. Its derivative converges quadratically against $0$ for large input values. On the other hand, in the sigmoid activation function, the derivative converges exponentially against $0$, which can cause problems during&amp;nbsp;back-propagation.&lt;/p&gt;
&lt;p&gt;Its graph is slightly flatter than $\tanh$, so it has a better tendency to differentiate between similar input&amp;nbsp;values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arctan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="arctan" src="https://adl1995.github.io/images/plots/arctan.svg"&gt;&lt;/p&gt;
&lt;h2&gt;LeCun&amp;#8217;s&amp;nbsp;Tanh&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = 1.7159 \tanh!\left( \frac{2}{3}&amp;nbsp;x^i_j\right)$$&lt;/p&gt;
&lt;p&gt;This activation function was first introduced in &lt;a href="http://yann.lecun.com/"&gt;Yann LeCun&lt;/a&gt;&amp;#8216;s paper &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"&gt;Efficient BackProp&lt;/a&gt;. The constants in the above equation have been chosen to keep the variance of the output close to $1$, because the gain of the sigmoid is roughly $1$ over its useful&amp;nbsp;range.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.7159&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="lecuns-tanh" src="https://adl1995.github.io/images/plots/lecuns-tanh.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Hard&amp;nbsp;Tanh&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = \max(-1, \min(1,&amp;nbsp;x^i_j))$$&lt;/p&gt;
&lt;p&gt;Compared to $\tanh$, the hard $\tanh$ activation function is computationally cheaper. It also saturates for magnitudes of $x$ greater than&amp;nbsp;$1$.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="hard-tanh" src="https://adl1995.github.io/images/plots/hard-tanh.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Sigmoid&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) =&amp;nbsp;\frac{1}{1+\exp(-x^i_j)}$$&lt;/p&gt;
&lt;p&gt;The sigmoid or logistic activation function maps the input values in the range $(0, 1)$, which is essentially their probability of belonging to a class. So, it is mostly used for multi-class&amp;nbsp;classification.&lt;/p&gt;
&lt;p&gt;However, like $\tanh$, it also suffers from the vanishing gradient problem. Also, its output is not zero-centered, which causes difficulties during the optimization step. It also has a low convergence&amp;nbsp;rate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="sigmoid" src="https://adl1995.github.io/images/plots/sigmoid.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Bipolar&amp;nbsp;Sigmoid&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) =&amp;nbsp;\frac{1-\exp(-x^i_j)}{1+\exp(-x^i_j)}$$&lt;/p&gt;
&lt;p&gt;The sigmoid function can be scaled to have any range of output values, depending upon the problem. When the range is from $-1$ to $1$, it is called a bipolar&amp;nbsp;sigmoid.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="bipolar-sigmoid" src="https://adl1995.github.io/images/plots/bipolar-sigmoid.svg"&gt;&lt;/p&gt;
&lt;h2&gt;ReLU (Rectified Linear&amp;nbsp;Unit)&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = \max(0,&amp;nbsp;x^i_j)$$&lt;/p&gt;
&lt;p&gt;A rectified linear unit has the output $0$ if its input is less than or equal to $0$, otherwise, its output is equal to its input. This activation function is also more &lt;a href="https://news.ycombinator.com/item?id=13338389"&gt;biologically accurate&lt;/a&gt;. It has been widely used in &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;convolutional neural networks&lt;/a&gt;. It is also superior to the sigmoid and $\tanh$ activation function, as it does not suffer from the vanishing gradient problem. Thus, it allows for faster and effective training of deep neural&amp;nbsp;architectures.&lt;/p&gt;
&lt;p&gt;However, being non-differentiable at $0$, ReLU neurons have the tendency to become inactive for all inputs, that is, they tend to die out. This can be caused by high learning rates, and can thus reduce the model&amp;#8217;s learning capacity. This is commonly referred to as the &amp;#8220;&lt;a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks"&gt;Dying ReLU&lt;/a&gt;&amp;#8221;&amp;nbsp;problem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="relu" src="https://adl1995.github.io/images/plots/relu.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Leaky&amp;nbsp;ReLU&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = \max(0.01 x^i_j,&amp;nbsp;x^i_j)$$&lt;/p&gt;
&lt;p&gt;The non-differentiability at zero problem can be solved by allowing a small value to flow when the input is less than or equal to $0$, which thus overcomes the &amp;#8220;Dying ReLU&amp;#8221; problem. It has proved to give better results for some&amp;nbsp;problems.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="leaky-relu" src="https://adl1995.github.io/images/plots/leaky-relu.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Smooth&amp;nbsp;ReLU&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) =&amp;nbsp;\log!\big(1+\exp(x^i_j)\big)$$&lt;/p&gt;
&lt;p&gt;Also known as the softplus unit, this activation function also overcomes the &amp;#8220;Dying ReLU&amp;#8221; problem by making itself differentiable everywhere and causes less saturation&amp;nbsp;overall.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="smooth-relu" src="https://adl1995.github.io/images/plots/smooth-relu.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Logit&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = \log!\bigg(\frac{x^i_j}{(1 −&amp;nbsp;x^i_j)}\bigg)$$&lt;/p&gt;
&lt;p&gt;This activation function performs the inverse operation of sigmoid,,that is, given probabilities in the range $(0, 1)$, it maps them to the full range of real numbers. The value of the logit function approaches infinity as the probability gets close to&amp;nbsp;$1$.&lt;/p&gt;
&lt;p&gt;It is mostly used in binary classification models, where we want to transform the binary input to real-valued&amp;nbsp;quantities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="logit" src="https://adl1995.github.io/images/plots/logit.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Softmax&lt;/h2&gt;
&lt;p&gt;$$a^i_j = f(x^i_j) = \frac{\exp(z^i_j)}{\sum\limits_k&amp;nbsp;\exp(z^i_k)}$$&lt;/p&gt;
&lt;p&gt;The softmax function gives us the probabilities that any of the classes are true. It produces values in the range $(0, 1)$. It also highlights the largest value and tries to suppress values which are below the maximum value; its resulting values always sum to $1$. This function is widely used in multiple classification logistic regression&amp;nbsp;models.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="softmax" src="https://adl1995.github.io/images/plots/softmax.svg"&gt;&lt;/p&gt;
&lt;p&gt;A &lt;a href="https://github.com/adl1995/adl1995.github.io/blob/master/notebooks/Activation%20functions.ipynb"&gt;Juptyer notebook&lt;/a&gt; containing all the above plots is hosted on&amp;nbsp;GitHub.&lt;/p&gt;</content><category term="Machine Learning"></category></entry></feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>/home/adeel - Machine Learning</title><link href="https://adl1995.github.io/" rel="alternate"></link><link href="https://adl1995.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://adl1995.github.io/</id><updated>2017-11-13T10:54:00+01:00</updated><entry><title>An overview of activation functions used in neural networks</title><link href="https://adl1995.github.io/an-overview-of-activation-functions-used-in-neural-networks.html" rel="alternate"></link><published>2017-11-13T10:54:00+01:00</published><updated>2017-11-13T10:54:00+01:00</updated><author><name>Adeel Ahmad</name></author><id>tag:adl1995.github.io,2017-11-13:/an-overview-of-activation-functions-used-in-neural-networks.html</id><summary type="html">&lt;p&gt;An activation function is used to introduce non-linearity in an artificial neural network. It allows us to model a class label or score that varies non-linearly with independent variables. Non-linearity means that the output cannot be replicated from a linear combination of inputs; this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;. On the other hand, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective only up to a single layer.&lt;/p&gt;
&lt;p&gt;To allow backpropagation through the network, the selected activation function should be differentiable. This property is required to compute â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;An activation function is used to introduce non-linearity in an artificial neural network. It allows us to model a class label or score that varies non-linearly with independent variables. Non-linearity means that the output cannot be replicated from a linear combination of inputs; this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;. On the other hand, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective only up to a single layer.&lt;/p&gt;
&lt;p&gt;To allow backpropagation through the network, the selected activation function should be differentiable. This property is required to compute the gradients which allows us to tune the network weights. The non-linear functions are continuous and transform the input (normally &lt;a href="http://cs231n.github.io/neural-networks-2/#datapre"&gt;zero-centered&lt;/a&gt;, however, these values get beyond their original scale once they get multiplied with their respective weights) in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, &lt;span class="math"&gt;\((-1, 1)\)&lt;/span&gt;, etc. In a neural network, it is possible for some neurons to have linear activation functions, but they must be accompanied by neurons with non-linear activation functions in some other part of the same network.&lt;/p&gt;
&lt;p&gt;Although any non-linear function can be used as an activation function, in practice, only a small fraction of these are used. The sections below describe various activation functions. These are accompanied with a Python snippet to plot them using &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; and &lt;a href="https://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;:&lt;/p&gt;
&lt;h2&gt;Binary step&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(z^i_j) = \begin{cases} 0  \hspace{1em} \text{if} \hspace{0.3em} z^i_j &amp;lt; 0 \\ 1 \hspace{1em} \text{if} \hspace{0.3em} z^i_j &amp;gt; 0 \end{cases}$$&lt;/div&gt;
&lt;p&gt;A binary step function is generally used in the &lt;a href="https://en.wikipedia.org/wiki/Perceptron"&gt;Perceptron linear classifier&lt;/a&gt;. It thresholds the input values to &lt;span class="math"&gt;\(1\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt;, if they are greater or less than zero, respectively.&lt;/p&gt;
&lt;p&gt;This activation function is useful when the input pattern can only belong to one or two groups, that is, binary classification.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="step" src="https://adl1995.github.io/images/plots/step.svg"&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \tanh(x^i_j)$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; non-linearity compresses the input in the range &lt;span class="math"&gt;\((-1, 1)\)&lt;/span&gt;. It provides an output which is zero-centered. So, large negative values are mapped to negative outputs. Similarly, zero-valued inputs are mapped to near zero outputs.&lt;/p&gt;
&lt;p&gt;Also, the gradients for &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; are steeper than sigmoid, but it suffers from the &lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"&gt;vanishing gradient problem&lt;/a&gt;. &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; is commonly referred to as the scaled version of sigmoid, and generally this equation holds: &lt;span class="math"&gt;\(\tanh(x) = 2 \sigma(2x) - 1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;An alternative equation for the &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; activation function is:&lt;/p&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{2}{1+\exp(-2x^i_j)} - 1$$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="tanh" src="https://adl1995.github.io/images/plots/tanh.svg"&gt;&lt;/p&gt;
&lt;h2&gt;ArcTan&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \tanh^{-1}(x^i_j)$$&lt;/div&gt;
&lt;p&gt;This activation function maps the input values in the range &lt;span class="math"&gt;\((-\pi/2, \pi/2)\)&lt;/span&gt;. Its derivative converges quadratically against &lt;span class="math"&gt;\(0\)&lt;/span&gt; for large input values. On the other hand, in the sigmoid activation function, the derivative converges exponentially against &lt;span class="math"&gt;\(0\)&lt;/span&gt;, which can cause problems during back-propagation.&lt;/p&gt;
&lt;p&gt;Its graph is slightly flatter than &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;, so it has a better tendency to differentiate between similar input values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arctan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="arctan" src="https://adl1995.github.io/images/plots/arctan.svg"&gt;&lt;/p&gt;
&lt;h2&gt;LeCun's Tanh&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = 1.7159 \tanh\!\left( \frac{2}{3} x^i_j\right)$$&lt;/div&gt;
&lt;p&gt;This activation function was first introduced in &lt;a href="http://yann.lecun.com/"&gt;Yann LeCun&lt;/a&gt;'s paper &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"&gt;Efficient BackProp&lt;/a&gt;. The constants in the above equation have been chosen to keep the variance of the output close to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, because the gain of the sigmoid is roughly &lt;span class="math"&gt;\(1\)&lt;/span&gt; over its useful range.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.7159&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="lecuns-tanh" src="https://adl1995.github.io/images/plots/lecuns-tanh.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Hard Tanh&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \max(-1, \min(1, x^i_j))$$&lt;/div&gt;
&lt;p&gt;Compared to &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;, the hard &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; activation function is computationally cheaper. It also saturates for magnitudes of &lt;span class="math"&gt;\(x\)&lt;/span&gt; greater than &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="hard-tanh" src="https://adl1995.github.io/images/plots/hard-tanh.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Sigmoid&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{1}{1+\exp(-x^i_j)}$$&lt;/div&gt;
&lt;p&gt;The sigmoid or logistic activation function maps the input values in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, which is essentially their probability of belonging to a class. So, it is mostly used for multi-class classification.&lt;/p&gt;
&lt;p&gt;However, like &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;, it also suffers from the vanishing gradient problem. Also, its output is not zero-centered, which causes difficulties during the optimization step. It also has a low convergence rate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="sigmoid" src="https://adl1995.github.io/images/plots/sigmoid.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Bipolar Sigmoid&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{1-\exp(-x^i_j)}{1+\exp(-x^i_j)}$$&lt;/div&gt;
&lt;p&gt;The sigmoid function can be scaled to have any range of output values, depending upon the problem. When the range is from &lt;span class="math"&gt;\(-1\)&lt;/span&gt; to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, it is called a bipolar sigmoid.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="bipolar-sigmoid" src="https://adl1995.github.io/images/plots/bipolar-sigmoid.svg"&gt;&lt;/p&gt;
&lt;h2&gt;ReLU (Rectified Linear Unit)&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \max(0, x^i_j)$$&lt;/div&gt;
&lt;p&gt;A rectified linear unit has the output &lt;span class="math"&gt;\(0\)&lt;/span&gt; if its input is less than or equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, otherwise, its output is equal to its input. This activation function is also more &lt;a href="https://news.ycombinator.com/item?id=13338389"&gt;biologically accurate&lt;/a&gt;. It has been widely used in &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;convolutional neural networks&lt;/a&gt;. It is also superior to the sigmoid and &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; activation function, as it does not suffer from the vanishing gradient problem. Thus, it allows for faster and effective training of deep neural architectures.&lt;/p&gt;
&lt;p&gt;However, being non-differentiable at &lt;span class="math"&gt;\(0\)&lt;/span&gt;, ReLU neurons have the tendency to become inactive for all inputs, that is, they tend to die out. This can be caused by high learning rates, and can thus reduce the model's learning capacity. This is commonly referred to as the "&lt;a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks"&gt;Dying ReLU&lt;/a&gt;" problem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="relu" src="https://adl1995.github.io/images/plots/relu.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Leaky ReLU&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \max(0.01 x^i_j, x^i_j)$$&lt;/div&gt;
&lt;p&gt;The non-differentiability at zero problem can be solved by allowing a small value to flow when the input is less than or equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, which thus overcomes the "Dying ReLU" problem. It has proved to give better results for some problems.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="leaky-relu" src="https://adl1995.github.io/images/plots/leaky-relu.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Smooth ReLU&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \log\!\big(1+\exp(x^i_j)\big)$$&lt;/div&gt;
&lt;p&gt;Also known as the softplus unit, this activation function also overcomes the "Dying ReLU" problem by making itself differentiable everywhere and causes less saturation overall.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="smooth-relu" src="https://adl1995.github.io/images/plots/smooth-relu.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Logit&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \log\!\bigg(\frac{x^i_j}{(1 âˆ’ x^i_j)}\bigg)$$&lt;/div&gt;
&lt;p&gt;This activation function performs the inverse operation of sigmoid,,that is, given probabilities in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, it maps them to the full range of real numbers. The value of the logit function approaches infinity as the probability gets close to &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is mostly used in binary classification models, where we want to transform the binary input to real-valued quantities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="logit" src="https://adl1995.github.io/images/plots/logit.svg"&gt;&lt;/p&gt;
&lt;h2&gt;Softmax&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{\exp(z^i_j)}{\sum\limits_k \exp(z^i_k)}$$&lt;/div&gt;
&lt;p&gt;The softmax function gives us the probabilities that any of the classes are true. It produces values in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;. It also highlights the largest value and tries to suppress values which are below the maximum value; its resulting values always sum to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This function is widely used in multiple classification logistic regression models.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="softmax" src="https://adl1995.github.io/images/plots/softmax.svg"&gt;&lt;/p&gt;
&lt;p&gt;A &lt;a href="https://github.com/adl1995/adl1995.github.io/blob/master/notebooks/Activation%20functions.ipynb"&gt;Juptyer notebook&lt;/a&gt; containing all the above plots is hosted on GitHub.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Machine Learning"></category></entry></feed>
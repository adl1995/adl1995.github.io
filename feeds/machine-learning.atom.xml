<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>/home/adeel - Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2017-11-13T10:54:00+01:00</updated><entry><title>An overview of activation functions used in neural networks</title><link href="/an-overview-of-activation-functions-used-in-neural-networks.html" rel="alternate"></link><published>2017-11-13T10:54:00+01:00</published><updated>2017-11-13T10:54:00+01:00</updated><author><name>Adeel Ahmad</name></author><id>tag:None,2017-11-13:/an-overview-of-activation-functions-used-in-neural-networks.html</id><summary type="html">&lt;p&gt;An activation function is used to introduce non-linearity to a network. This allows us to model a class label / score that varies non-linearly with independent variables. Non-linear means the output cannot be replicated from a linear combination of inputs, this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;, whereas, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective for only a single layer.&lt;/p&gt;
&lt;p&gt;Another important aspect of the activation function is that it should be differentiable. This is required when we backpropagate through our network and compute gradients, and thus …&lt;/p&gt;</summary><content type="html">&lt;p&gt;An activation function is used to introduce non-linearity to a network. This allows us to model a class label / score that varies non-linearly with independent variables. Non-linear means the output cannot be replicated from a linear combination of inputs, this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;, whereas, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective for only a single layer.&lt;/p&gt;
&lt;p&gt;Another important aspect of the activation function is that it should be differentiable. This is required when we backpropagate through our network and compute gradients, and thus tune our weights accordingly. The non-linear functions are continuous and transform the input (normally &lt;a href="http://cs231n.github.io/neural-networks-2/#datapre"&gt;zero-centered&lt;/a&gt;, however, these values get beyond their original scale once they get multiplied with their respective weights) in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, &lt;span class="math"&gt;\((-1, 1)\)&lt;/span&gt;, etc. In a neural network, it is possible for some neurons to have linear activation functions, but they must be accompanied by neurons with non-linear activation functions in some other part of the same network.&lt;/p&gt;
&lt;p&gt;Although any non-linear function can be used as an activation function, in practice, only a small fraction of these are used. Listed below are some commonly used activation functions along with a Python snippet to create their plot using &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; and &lt;a href="https://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;:&lt;/p&gt;
&lt;h2&gt;Binary step&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(z^i_j) = \begin{cases} 0  \hspace{1em} \text{if} \hspace{0.3em} z^i_j &amp;lt; 0 \\ 1 \hspace{1em} \text{if} \hspace{0.3em} z^i_j &amp;gt; 0 \end{cases}$$&lt;/div&gt;
&lt;p&gt;A binary step function is generally used in the &lt;a href="https://en.wikipedia.org/wiki/Perceptron"&gt;Perceptron linear classifier&lt;/a&gt;. It thresholds the input values to &lt;span class="math"&gt;\(1\)&lt;/span&gt; and &lt;span class="math"&gt;\(0\)&lt;/span&gt;, if they are greater or less than zero, respectively.&lt;/p&gt;
&lt;p&gt;This activation function is useful when the input pattern can only belong to one of two groups i.e. binary classification.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="step" src="/images/plots/step.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \tanh(x^i_j)$$&lt;/div&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; non-linearity compresses the input in the range &lt;span class="math"&gt;\((-1, 1)\)&lt;/span&gt;. It provides an output which is zero-centered. So, large negative values are mapped to negative outputs, similarly, zero-valued inputs are mapped to near zero outputs.&lt;/p&gt;
&lt;p&gt;Also, the gradients for &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; are steeper than sigmoid, but it suffers from the &lt;a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"&gt;vanishing gradient problem&lt;/a&gt;. &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; is commonly referred to as the scaled version of sigmoid, generally this equation holds: &lt;span class="math"&gt;\(\tanh(x) = 2 \sigma(2x) - 1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;An alternative equation for the &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; activation function is:&lt;/p&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{2}{1+\exp(-2x^i_j)} - 1$$&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="tanh" src="/images/plots/tanh.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;ArcTan&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \tanh^{-1}(x^i_j)$$&lt;/div&gt;
&lt;p&gt;This activation function maps the input values in the range &lt;span class="math"&gt;\((-\pi/2, \pi/2)\)&lt;/span&gt;. Its derivative converges quadratically against zero for large input values. On the other hand, in the sigmoid activation function, the derivative converges exponentially against zero, which can cause problems during back-propagation.&lt;/p&gt;
&lt;p&gt;Its graph is slightly flatter than &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;, so it has a better tendency to differentiate between similar input values.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arctan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="arctan" src="/images/plots/arctan.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;LeCun’s Tanh&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = 1.7159 \tanh\!\left( \frac{2}{3} x^i_j\right)$$&lt;/div&gt;
&lt;p&gt;This activation function was first introduced in &lt;a href="http://yann.lecun.com/"&gt;Yann LeCun&lt;/a&gt;‘s paper &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"&gt;Efficient BackProp&lt;/a&gt;. The constants in the above equation have been chosen to keep the variance of the output close to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, because the gain of sigmoid is roughly &lt;span class="math"&gt;\(1\)&lt;/span&gt; over its useful range.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.7159&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="lecuns-tanh" src="/images/plots/lecuns-tanh.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Hard Tanh&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \max(-1, \min(1, x^i_j))$$&lt;/div&gt;
&lt;p&gt;Compared to &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;, the hard &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; activation function is computationally cheaper. It also saturates for magnitudes of &lt;span class="math"&gt;\(x\)&lt;/span&gt; greater than &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="hard-tanh" src="/images/plots/hard-tanh.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Sigmoid&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{1}{1+\exp(-x^i_j)}$$&lt;/div&gt;
&lt;p&gt;The sigmoid or logistic activation function maps the input values in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, which is essentially their probability of belonging to a class. So, it is mostly used for multi-class classification.&lt;/p&gt;
&lt;p&gt;However, like &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt;, it also suffers from the vanishing gradient problem. Also, the output it produces is not zero-centered, which causes difficulties during optimization. It also has a low convergence rate.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="sigmoid" src="/images/plots/sigmoid.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Bipolar Sigmoid&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{1-\exp(-x^i_j)}{1+\exp(-x^i_j)}$$&lt;/div&gt;
&lt;p&gt;The sigmoid function can be scaled to have any range of output values, depending upon the problem. When the range is from &lt;span class="math"&gt;\(-1\)&lt;/span&gt; to &lt;span class="math"&gt;\(1\)&lt;/span&gt;, it is called a bipolar sigmoid.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="bipolar-sigmoid" src="/images/plots/bipolar-sigmoid.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;ReLU (Rectified Linear Unit)&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \max(0, x^i_j)$$&lt;/div&gt;
&lt;p&gt;A rectified linear unit has the output &lt;span class="math"&gt;\(0\)&lt;/span&gt; if its input is less than or equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, otherwise, its output is equal to its input. It is also more &lt;a href="https://news.ycombinator.com/item?id=13338389"&gt;biologically accurate&lt;/a&gt;. This has been widely used in &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;convolutional neural networks&lt;/a&gt;. It is also superior to the sigmoid and &lt;span class="math"&gt;\(\tanh\)&lt;/span&gt; activation function, as it does not suffer from the vanishing gradient problem. Thus, it allows for faster and effective training of deep neural architectures.&lt;/p&gt;
&lt;p&gt;However, being non-differentiable at &lt;span class="math"&gt;\(0\)&lt;/span&gt;, ReLU neurons have a tendency to become inactive for all inputs i.e. they die out. This can be caused by high learning rates, and can thus reduce the model’s learning capacity. This is commonly referred to as the “&lt;a href="https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks"&gt;Dying ReLU&lt;/a&gt;” problem.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="relu" src="/images/plots/relu.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Leaky ReLU&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \max(0.01 x^i_j, x^i_j)$$&lt;/div&gt;
&lt;p&gt;The non-differentiability at zero problem can be solved by allowing a small value to flow when the input is less than or equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, which thus overcomes the “Dying ReLU” problem. It has proved to give better results for some problems.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="leaky-relu" src="/images/plots/leaky-relu.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Smooth ReLU&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \log\!\big(1+\exp(x^i_j)\big)$$&lt;/div&gt;
&lt;p&gt;Also known as the softplus unit, this activation function also overcomes the “Dying ReLU” problem by making itself differentiable everywhere and causes less saturation overall.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="smooth-relu" src="/images/plots/smooth-relu.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Logit&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \log\!\bigg(\frac{x^i_j}{(1 − x^i_j)}\bigg)$$&lt;/div&gt;
&lt;p&gt;This function performs the inverse operation of sigmoid i.e. given probabilities in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, it maps them to full range of real numbers. The value of the logit function approaches infinity as the probability gets close to &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is mostly used in binary classification models, where we want to transform the binary inputs to real-valued quantities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="logit" src="/images/plots/logit.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;h2&gt;Softmax&lt;/h2&gt;
&lt;div class="math"&gt;$$a^i_j = f(x^i_j) = \frac{\exp(z^i_j)}{\sum\limits_k \exp(z^i_k)}$$&lt;/div&gt;
&lt;p&gt;The softmax function’s output tells us the probabilities that any of the classes are true, so it produces values in the range &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;. It highlights the largest values and tries to suppress values which are below the maximum value, its resulting values always sum to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. This function is widely used in multiple classification logistic regression models.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="softmax" src="/images/plots/softmax.svg" style="width: 100%; height: auto; max-width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;A &lt;a href="https://github.com/adl1995/adl1995.github.io/blob/master/notebooks/Activation%20functions.ipynb"&gt;Juptyer notebook&lt;/a&gt; containing all the above plots is hosted on GitHub.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry></feed>
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>/home/adeel - Machine Learning</title><link>/</link><description></description><lastBuildDate>Mon, 13 Nov 2017 10:54:00 +0100</lastBuildDate><item><title>An overview of activation functions used in neural networks</title><link>/an-overview-of-activation-functions-used-in-neural-networks.html</link><description>&lt;p&gt;An activation function is used to introduce non-linearity in an artificial neural network. It allows us to model a class label or score that varies non-linearly with independent variables. Non-linearity means that the output cannot be replicated from a linear combination of inputs; this allows the model to learn complex mappings from the available data, and thus the network becomes a &lt;a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem"&gt;universal approximator&lt;/a&gt;. On the other hand, a model which uses a linear function (i.e. no activation function) is unable to make sense of complicated data, such as, speech, videos, etc. and is effective only up to a single layer.&lt;/p&gt;
&lt;p&gt;To allow backpropagation through the network, the selected activation function should be differentiable. This property is required to compute â€¦&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Adeel Ahmad</dc:creator><pubDate>Mon, 13 Nov 2017 10:54:00 +0100</pubDate><guid isPermaLink="false">tag:None,2017-11-13:/an-overview-of-activation-functions-used-in-neural-networks.html</guid><category>Machine Learning</category></item></channel></rss>